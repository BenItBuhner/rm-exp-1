# Athena v2 TPU training config
# Optimized for TPU v3-8 (8 cores)

defaults:
  - arch: athena_trm_large
  - _self_

hydra:
  output_subdir: null

data_paths: ["gs://your-bucket/athena_data/athena_base"]  # Use GCS for best TPU performance
data_paths_test: []

evaluators:
  - name: text@PerplexityEvaluator

# TPU-optimized batch size (larger is better for TPU)
global_batch_size: 256  # 32 per core on v3-8
epochs: 100000
eval_interval: 5000
min_eval_interval: 1
checkpoint_every_eval: true

lr: 8e-5
lr_min_ratio: 0.1
lr_warmup_steps: 4000

beta1: 0.9
beta2: 0.95
weight_decay: 0.1
grad_clip_norm: 1.0
puzzle_emb_weight_decay: 0.1

puzzle_emb_lr: 2e-4

seed: 0

ema: true
ema_rate: 0.9995
freeze_weights: false

# Force TPU usage (will error if TPU not available)
device_type: "tpu"

eval_save_outputs: ["logits", "reasoning_logits"]

